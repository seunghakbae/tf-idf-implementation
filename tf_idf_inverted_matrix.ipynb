{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 document를 저장할 수 있는 text_set 선언\n",
    "text_set = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open the folder where documents are saved, and save all the documents\n",
    "path = 'data/'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    file = open(os.path.join(path,filename), 'r', encoding='utf-8')\n",
    "    text = file.readlines()\n",
    "    text_set.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' (CNN)If you remember Miley twerking or Kanye\\'s mic-snatching, you\\'ll know music awards can get chaotic.Like on Tuesday, when singer Taylor Swift took a Twitter dig at hip-hop queen Nicki Minaj and the Internet erupted into a sprawling conversation about race and gender. And then Kim Kardashian got involved. Stay with us.\\'Black women are rarely rewarded\\'It started when Minaj took to her keyboard to vent after one of her biggest hits wasn\\'t nominated for video of the year for the MTV Video Music Awards.Mind you, this isn\\'t any old clip. \"Anaconda,\" an uproarious, raunchy twist on pop culture portrayals of curvy black women, received 19.6 million clicks in its first 24 hours, a music video record at the time. So the rapper had a reason to be upset when she was snubbed -- and she made her reasoning clear.\"If I was a different \\'kind\\' of artist, Anaconda would be nominated for best choreo and vid of the year as well,\" said Minaj.\"Black women influence pop culture so much but are rarely rewarded for it.\"She then tweeted: \"If your video celebrates women with very slim bodies, you will be nominated for vid of the year.\"That\\'s when Taylor Swift -- who describes herself as a feminist -- jumped in, apparently taking the tweets personally.\"I\\'ve done nothing but love & support you. It\\'s unlike you to pit women against each other. Maybe one of the men took your slot,\" she wrote.Unlike \"Anaconda,\" Swift\\'s video for \"Bad Blood\" was nominated for the top award. Featuring whisper-thin supermodels wielding guns, the pseudo-action movie dethroned Minaj\\'s video for most views in one day when it was released this year.It was a surprisingly barbed volley from a singer known for her meticulously crafted public image -- even Nicki Minaj seemed taken aback. \"Didn\\'t say a word about u,\" she replied.The rapper then retweeted a supporter who accused Taylor Swift of not being \"critical of racist media.\"Swift followed up with what seemed to be an attempt to smooth things over. \"You\\'re invited to any stage I\\'m ever on,\" she said.Miley Cyrus to host VMAsA double standard?Twitter wasn\\'t buying it: As \"Nicki\" and \"Taylor\" became worldwide trends, thousands of users came to Nicki Minaj\\'s defense.Many said the stars\\' exchange showed a double standard: Swift was widely praised as brave for speaking up about problems in the music industry last month -- now Minaj was getting lashed for doing the same. Nicki was making an important point, they argued: Not all women\\'s experiences are the same, but black women don\\'t often get taken as seriously.Users expressed disapproval with the contrasting way some media portrayed the respective artists: Taylor Swift -- beautiful, pure; Nicki Minaj -- hysterical, angry.Janet Mock, a writer and trans woman activist, suggested flipping the script, posting a picture of an angry Swift and a calm Minaj.It wasn\\'t long before at least one publication -- Entertainment Weekly -- apologized for their \"insensitive juxtaposition of photos.\"For many users, Swift and Minaj\\'s interaction reflected an all-too-familiar dynamic: A black woman makes a point, and a white woman tells her to simmer down or hijacks the conversation in the name of feminism.\"A black woman speaks up on racial problems within the music industry. A white woman makes it completely about herself,\" said user XLNB.Supporters of Taylor Swift began defending the singer with the hashtag #TeamTaylor. Some argued Beyonce\\'s nomination meant Nicki\\'s omission couldn\\'t have been about race.But at day\\'s end there were more than double the amount of tweets using the hashtag #TeamNicki, according to Topsy, a Twitter analytics site.Kim Kardashian, Bruno Mars join the frayThen, the conversation took a slightly absurd turn as Kim Kardashian waded into the story -- unwittingly, she claims.As the Nicki and Taylor saga continued to ride high in Twitter\\'s trends, the reality show star tweeted a photo of herself with the caption, \"Imma let you finish but...\"For those of you who missed the reference: Kanye West -- Kim Kardashian\\'s husband -- uttered that phrase when he grabbed the microphone from Taylor Swift at the MTV Music Video Awards in 2009. So it sounded like Kim was telling Taylor to back off. Something she frantically denied once users started pointing out the connection.\"Wait wait I\\'m in Paris it\\'s the middle of the night & I\\'m posting my Vogue Spain pics not having a clue what\\'s going on in the music world,\" she wrote.If that weren\\'t enough drama for an evening, rapper Meek Mill, Nicki Minaj\\'s boyfriend, decided before midnight that it would be a perfect time to launch an attack on superstar hip-hop artist Drake.But in the end, it was mild-mannered Bruno Mars, a video of the year nominee, who may have won the night, with a joking provocation of fellow nominee Ed Sheeran.\"Yo I want in on this twitter Beef!! VMAs is the new WWF!! @edsheeran F*** You!\" he said.And that is how the Internet works.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Flow\n",
    "\n",
    "### 1. 각 text별로 tokenize를 한다. \n",
    "### 2. token에서 stopwords나 필요없는 부분을 제거하고 stemming을 해준다. \n",
    "### 3. 각 document, 각 token별 token 몇개 있는지 구한 다음, table로 저장한다. \n",
    "### 4. 각 token별 어떤 document가 있는 지 inverted index를 구한다. \n",
    "### 5. query가 속한 document들만 가져온다. \n",
    "### 6. tf-idf를 구한다.\n",
    "### 7. query의 tf-idf를 구한다. \n",
    "### 8. cosine-similarity를 구한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords 불러오기.\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.add('\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.add('\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.add(\"!\")\n",
    "stop.add(\"?\")\n",
    "stop.add(\"@\")\n",
    "stop.add(\"*\")\n",
    "stop.add(\"^\")\n",
    "stop.add(\"\\\"\\\"\")\n",
    "stop.add(\"\\'\\'\")\n",
    "stop.add(\"#\")\n",
    "stop.add(\"$\")\n",
    "stop.add(\"%\")\n",
    "stop.add(\"(\")\n",
    "stop.add(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-2c4413184a9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 소문자로 만든 뒤 tokenized를 했다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mporter_stemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# stopwords를 삭제했다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mvocab_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-2c4413184a9b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 소문자로 만든 뒤 tokenized를 했다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mporter_stemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# stopwords를 삭제했다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mvocab_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_step3\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    538\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[1;34m'ical'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[1;34m'ful'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[1;34m'ness'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m             ],\n\u001b[0;32m    542\u001b[0m         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[1;34m(self, word, rules)\u001b[0m\n\u001b[0;32m    265\u001b[0m                     \u001b[1;31m# Don't try any further rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(text_set)):\n",
    "    for j in range(len(text_set[i][0])):\n",
    "        tokenized = word_tokenize(text_set[i][0].lower()) # 소문자로 만든 뒤 tokenized를 했다. \n",
    "        tokenized = [porter_stemmer.stem(token) for token in tokenized if token not in stop] # stopwords를 삭제했다. \n",
    "        vocab_list.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_set\n",
    "vocab_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-eedea1f79218>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# token에서 stopwords를 제거하고 \n",
    "for i in range(len(text_set)):\n",
    "    for j in range(len(text_set[i][0])):\n",
    "        tokenized = word_tokenize(text_set[i][0])\n",
    "        for token in tokenized:\n",
    "            if token not in stop:\n",
    "                vocab_set.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '#',\n",
       " '&',\n",
       " \"''\",\n",
       " \"'90s\",\n",
       " \"'Black\",\n",
       " \"'Lemonade\",\n",
       " \"'kind\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '--',\n",
       " '.',\n",
       " '...',\n",
       " '1',\n",
       " '11',\n",
       " '11:30',\n",
       " '12',\n",
       " '14,000',\n",
       " '15',\n",
       " '19.6',\n",
       " '1992',\n",
       " '1997',\n",
       " '2009',\n",
       " '2012',\n",
       " '2015',\n",
       " '2017',\n",
       " '2018',\n",
       " '22.Beyoncé',\n",
       " '24',\n",
       " '25',\n",
       " '25th',\n",
       " '27',\n",
       " '30-something',\n",
       " '59th',\n",
       " '6',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'AND',\n",
       " 'Abel',\n",
       " 'Academy',\n",
       " 'Adele',\n",
       " 'Album',\n",
       " 'All',\n",
       " 'Anaconda',\n",
       " 'And',\n",
       " 'Angeles',\n",
       " 'Anniversary',\n",
       " 'April',\n",
       " 'Arts',\n",
       " 'As',\n",
       " 'Ask',\n",
       " 'At',\n",
       " 'Awards',\n",
       " 'Awards.Mind',\n",
       " 'B',\n",
       " 'Bad',\n",
       " 'Beck',\n",
       " 'Beef',\n",
       " 'Before',\n",
       " 'Bette',\n",
       " 'Beyonce',\n",
       " 'Beyonce.Though',\n",
       " 'Beyoncé',\n",
       " 'Beyoncé.Lady',\n",
       " 'Big',\n",
       " 'Black',\n",
       " 'Blige',\n",
       " 'Blood',\n",
       " 'Bon',\n",
       " 'Bowl',\n",
       " 'British',\n",
       " 'Bruno',\n",
       " 'Butterfly',\n",
       " 'CBS',\n",
       " 'CNN',\n",
       " 'Ca',\n",
       " 'Can',\n",
       " 'Canadian',\n",
       " 'Center',\n",
       " 'Chris',\n",
       " 'Christina',\n",
       " 'Coachella',\n",
       " 'Coldplay',\n",
       " 'Cologne',\n",
       " 'Come',\n",
       " 'Cool',\n",
       " 'Country',\n",
       " 'Crush',\n",
       " 'Cue',\n",
       " 'Cyrus',\n",
       " 'Did',\n",
       " 'Drake.But',\n",
       " 'ET',\n",
       " 'Ed',\n",
       " 'Ehrlich.Last',\n",
       " 'Entertainment',\n",
       " 'Everywhere',\n",
       " 'Expect',\n",
       " 'F***',\n",
       " 'Face',\n",
       " 'Fastlove',\n",
       " 'Featuring',\n",
       " 'February',\n",
       " 'Feel',\n",
       " 'Festival',\n",
       " 'First',\n",
       " 'For',\n",
       " 'Gaga',\n",
       " 'George',\n",
       " 'Germany',\n",
       " 'Girl',\n",
       " 'Grammy',\n",
       " 'Grammys',\n",
       " 'Greatest',\n",
       " 'Hanson',\n",
       " 'Hanson-related',\n",
       " 'Hello',\n",
       " 'Hits',\n",
       " 'Houston.The',\n",
       " 'I',\n",
       " 'IT',\n",
       " 'If',\n",
       " 'Imma',\n",
       " 'In',\n",
       " 'Internet',\n",
       " 'Is',\n",
       " 'Isaac',\n",
       " 'It',\n",
       " 'Iver',\n",
       " 'J',\n",
       " 'J.',\n",
       " 'Jay-Z',\n",
       " 'June',\n",
       " 'June.The',\n",
       " 'Just',\n",
       " 'Justin',\n",
       " 'Kanye',\n",
       " 'Kardashian',\n",
       " 'Ken',\n",
       " 'Kendrick',\n",
       " 'Kim',\n",
       " 'LL',\n",
       " 'Lady',\n",
       " 'Lamar',\n",
       " 'Lemonade',\n",
       " 'Let',\n",
       " 'Lionel',\n",
       " 'Little',\n",
       " 'Lorde',\n",
       " 'Los',\n",
       " 'Love',\n",
       " 'MMMBop',\n",
       " 'MMMhops',\n",
       " 'MTV',\n",
       " 'Mars',\n",
       " 'Martin',\n",
       " 'Mary',\n",
       " 'Maybe',\n",
       " 'Meek',\n",
       " 'Michael',\n",
       " 'Middle',\n",
       " 'Milder',\n",
       " 'Miley',\n",
       " 'Mill',\n",
       " 'Minaj',\n",
       " 'Minaj.It',\n",
       " 'Mock',\n",
       " 'Monster',\n",
       " 'Mother',\n",
       " 'Music',\n",
       " 'My',\n",
       " 'NAIL',\n",
       " 'Neil',\n",
       " 'Nicki',\n",
       " 'No',\n",
       " 'Not',\n",
       " 'Nowhere',\n",
       " 'OVER',\n",
       " 'October',\n",
       " 'Of',\n",
       " 'Paris',\n",
       " 'Perri.Adele',\n",
       " 'Pimp',\n",
       " 'Pitchfork',\n",
       " 'Portnow',\n",
       " 'President',\n",
       " 'Radiohead',\n",
       " 'Recording',\n",
       " 'Richie',\n",
       " 'START',\n",
       " 'Sam',\n",
       " 'She',\n",
       " 'Sheeran',\n",
       " 'Smith',\n",
       " 'So',\n",
       " 'Some',\n",
       " 'Something',\n",
       " 'Spain',\n",
       " 'Staples',\n",
       " 'Stay',\n",
       " 'Sunday',\n",
       " 'Super',\n",
       " 'Swift',\n",
       " 'TV',\n",
       " 'Taylor',\n",
       " 'TeamNicki',\n",
       " 'TeamTaylor',\n",
       " 'Tesfaye',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Then',\n",
       " 'This',\n",
       " 'Thursday',\n",
       " 'Timberlake',\n",
       " 'To',\n",
       " 'Topsy',\n",
       " 'Tour',\n",
       " 'Town',\n",
       " 'Tuesday',\n",
       " 'Twitter',\n",
       " 'VMAs',\n",
       " 'VMAsA',\n",
       " 'Valley',\n",
       " 'Video',\n",
       " 'Visit',\n",
       " 'Vogue',\n",
       " 'WWF',\n",
       " 'Wait',\n",
       " 'We',\n",
       " 'Wednesday',\n",
       " 'Weekly',\n",
       " 'Weeknd',\n",
       " 'West',\n",
       " 'When',\n",
       " 'While',\n",
       " 'Will',\n",
       " 'XLNB.Supporters',\n",
       " 'Year',\n",
       " 'Yo',\n",
       " 'You',\n",
       " 'Zac',\n",
       " '[',\n",
       " ']',\n",
       " '``',\n",
       " 'aback',\n",
       " 'absurd',\n",
       " 'acceptance',\n",
       " 'acclaimed',\n",
       " 'according',\n",
       " 'accused',\n",
       " 'activist',\n",
       " 'added',\n",
       " 'additional',\n",
       " 'advice',\n",
       " 'aforementioned',\n",
       " 'age.He',\n",
       " 'ago',\n",
       " 'air',\n",
       " 'album',\n",
       " 'album.The',\n",
       " 'albums',\n",
       " 'all-too-familiar',\n",
       " 'almost',\n",
       " 'also',\n",
       " 'always',\n",
       " 'ambitious',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'analytics',\n",
       " 'angry',\n",
       " 'angry.Janet',\n",
       " 'announced',\n",
       " 'annual',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'apologizing',\n",
       " 'apology',\n",
       " 'apparently',\n",
       " 'appeared',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'argued',\n",
       " 'artist',\n",
       " 'artistry',\n",
       " 'artists',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attempting',\n",
       " 'audience',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awry',\n",
       " 'back',\n",
       " 'backerWest',\n",
       " 'band',\n",
       " 'barbed',\n",
       " 'based',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'become',\n",
       " 'beer',\n",
       " 'began',\n",
       " 'behind',\n",
       " 'best',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'black',\n",
       " 'blindfold',\n",
       " 'bodies',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'brave',\n",
       " 'brief',\n",
       " 'briefly',\n",
       " 'brothers',\n",
       " 'buying',\n",
       " 'buzz',\n",
       " 'ca',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'caption',\n",
       " 'case',\n",
       " 'catalog',\n",
       " 'causing',\n",
       " 'celebrates',\n",
       " 'censor',\n",
       " 'censors',\n",
       " 'chanteuse',\n",
       " 'chaotic.Like',\n",
       " 'chart-busting',\n",
       " 'cheered',\n",
       " 'child',\n",
       " 'children',\n",
       " 'choreo',\n",
       " 'claims.As',\n",
       " 'clear',\n",
       " 'clicks',\n",
       " 'clip',\n",
       " 'clue',\n",
       " 'co-produced.There',\n",
       " 'collective',\n",
       " 'college',\n",
       " 'compilation',\n",
       " 'completely',\n",
       " 'confirmed',\n",
       " 'connection',\n",
       " 'consecutive',\n",
       " 'contentious',\n",
       " 'continued',\n",
       " 'contrasting',\n",
       " 'controversially',\n",
       " 'conversation',\n",
       " 'could',\n",
       " 'country',\n",
       " 'crafted',\n",
       " 'crazy',\n",
       " 'create',\n",
       " 'creative',\n",
       " 'critical',\n",
       " 'critics',\n",
       " 'crowd',\n",
       " 'culture',\n",
       " 'current',\n",
       " 'curvy',\n",
       " 'cut-off',\n",
       " 'dads',\n",
       " 'dates',\n",
       " 'day',\n",
       " 'days',\n",
       " 'decided',\n",
       " 'declared',\n",
       " 'dedicated',\n",
       " 'defending',\n",
       " 'defense.Many',\n",
       " 'denied',\n",
       " 'describes',\n",
       " 'desert',\n",
       " 'determines',\n",
       " 'dethroned',\n",
       " 'different',\n",
       " 'dig',\n",
       " 'disapproval',\n",
       " 'discussion',\n",
       " 'dismisses',\n",
       " 'disses',\n",
       " 'distracting',\n",
       " 'diverse',\n",
       " 'doctors',\n",
       " 'done',\n",
       " 'double',\n",
       " 'drama',\n",
       " 'drawn',\n",
       " 'drop-out',\n",
       " 'dropped',\n",
       " 'duo/group',\n",
       " 'duties',\n",
       " 'dynamic',\n",
       " 'earlier',\n",
       " 'earned',\n",
       " 'earworm',\n",
       " 'edsheeran',\n",
       " 'election',\n",
       " 'electoral',\n",
       " 'eligible',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'enough',\n",
       " 'erupted',\n",
       " 'ethnicity',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'ever',\n",
       " 'example',\n",
       " 'exchange',\n",
       " 'excited',\n",
       " 'expectant',\n",
       " 'experiences',\n",
       " 'express',\n",
       " 'expressed',\n",
       " 'fans',\n",
       " 'favorite',\n",
       " 'feature',\n",
       " 'fell',\n",
       " 'fellow',\n",
       " 'feminism',\n",
       " 'feminist',\n",
       " 'festival',\n",
       " 'fifth',\n",
       " 'finish',\n",
       " 'finishing',\n",
       " 'first',\n",
       " 'five',\n",
       " 'flipping',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'formed',\n",
       " 'frantically',\n",
       " 'frayThen',\n",
       " 'front',\n",
       " 'full',\n",
       " 'gender',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'gifted',\n",
       " 'girl',\n",
       " 'give',\n",
       " 'given',\n",
       " 'go',\n",
       " 'going',\n",
       " 'gone.Hanson',\n",
       " 'good',\n",
       " 'got',\n",
       " 'grab',\n",
       " 'grabbed',\n",
       " 'great',\n",
       " 'group',\n",
       " 'guns',\n",
       " 'halftime',\n",
       " 'hard',\n",
       " 'hashtag',\n",
       " 'hate',\n",
       " 'headline',\n",
       " 'headliners',\n",
       " 'heads',\n",
       " 'hear',\n",
       " 'high',\n",
       " 'hijacks',\n",
       " 'hip-hop',\n",
       " 'hit',\n",
       " 'hits',\n",
       " 'hook',\n",
       " 'host',\n",
       " 'hours',\n",
       " 'hugging',\n",
       " 'humble',\n",
       " 'husband',\n",
       " 'hysterical',\n",
       " 'image',\n",
       " 'immediate',\n",
       " 'important',\n",
       " 'incident',\n",
       " 'include',\n",
       " 'included',\n",
       " 'including',\n",
       " 'industry',\n",
       " 'influence',\n",
       " 'inherently',\n",
       " 'initial',\n",
       " 'inner',\n",
       " 'insensitive',\n",
       " 'instead',\n",
       " 'instrument',\n",
       " 'interaction',\n",
       " 'interview',\n",
       " 'invited',\n",
       " 'involved',\n",
       " 'join',\n",
       " 'joined',\n",
       " 'joking',\n",
       " 'jumped',\n",
       " 'juxtaposition',\n",
       " 'keyboard',\n",
       " 'kicked',\n",
       " 'kicking',\n",
       " 'know',\n",
       " 'known',\n",
       " 'lashed',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'launch',\n",
       " 'launched',\n",
       " 'least',\n",
       " 'lesson',\n",
       " 'let',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'list',\n",
       " 'listen',\n",
       " 'lists',\n",
       " 'live',\n",
       " 'long',\n",
       " 'loss',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'made',\n",
       " 'make',\n",
       " 'make-up',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'many',\n",
       " 'massive',\n",
       " 'may',\n",
       " 'means',\n",
       " 'meant',\n",
       " 'media',\n",
       " 'members',\n",
       " 'men',\n",
       " 'merchandise',\n",
       " 'mess',\n",
       " 'meticulously',\n",
       " 'mic-snatching',\n",
       " 'microphone',\n",
       " 'mics',\n",
       " 'middle',\n",
       " 'midnight',\n",
       " 'mild-mannered',\n",
       " 'million',\n",
       " 'missed',\n",
       " 'moment',\n",
       " 'moments',\n",
       " 'month',\n",
       " 'monumental',\n",
       " 'mother',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'music',\n",
       " 'musicians',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'necessarily',\n",
       " 'need',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'night',\n",
       " 'nominated',\n",
       " 'nomination',\n",
       " 'nominations',\n",
       " 'nominee',\n",
       " 'nostalgia',\n",
       " 'nothing',\n",
       " 'now.Hanson',\n",
       " 'objectivity',\n",
       " 'offended',\n",
       " 'offered',\n",
       " 'often',\n",
       " 'ok',\n",
       " 'old',\n",
       " 'omission',\n",
       " 'one',\n",
       " 'onto',\n",
       " 'opinion',\n",
       " 'organization',\n",
       " 'others',\n",
       " 'ovation',\n",
       " 'overlay',\n",
       " 'p.m',\n",
       " 'paired',\n",
       " 'party',\n",
       " 'past',\n",
       " 'paused',\n",
       " 'perceived',\n",
       " 'perfect',\n",
       " 'perform',\n",
       " 'performance',\n",
       " 'performances.Lamar',\n",
       " 'performer.Both',\n",
       " 'performers',\n",
       " 'performing',\n",
       " 'personally',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'phrase',\n",
       " 'piano',\n",
       " 'picked',\n",
       " 'pics',\n",
       " 'picture',\n",
       " 'piece',\n",
       " 'pile',\n",
       " 'pit',\n",
       " 'pitch-perfect',\n",
       " 'plagued',\n",
       " 'plans',\n",
       " 'played',\n",
       " 'please',\n",
       " 'point',\n",
       " 'pointing',\n",
       " 'pop',\n",
       " 'pop/R',\n",
       " 'popular',\n",
       " 'portrayals',\n",
       " 'portrayed',\n",
       " 'posting',\n",
       " 'powered',\n",
       " 'praised',\n",
       " 'pregnancy-prompted',\n",
       " 'pregnant',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'producer',\n",
       " 'prompting',\n",
       " 'propsBruno',\n",
       " 'provocation',\n",
       " 'pseudo-action',\n",
       " 'public',\n",
       " 'publication',\n",
       " 'publicly',\n",
       " 'pure',\n",
       " 'put',\n",
       " 'queen',\n",
       " 'questioned',\n",
       " 'race',\n",
       " 'race.But',\n",
       " 'racial',\n",
       " 'racism',\n",
       " 'racist',\n",
       " 'rapper',\n",
       " 'rapper-actor',\n",
       " 'rarely',\n",
       " 'ratings',\n",
       " 'raunchy',\n",
       " 'raves',\n",
       " 'reached',\n",
       " 'read',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasoning',\n",
       " 'received',\n",
       " 'receives',\n",
       " 'recent',\n",
       " 'recipient',\n",
       " 'record',\n",
       " 'reference',\n",
       " 'referenced',\n",
       " 'reflected',\n",
       " 'released',\n",
       " 'releasing',\n",
       " 'remember',\n",
       " 'remorse',\n",
       " 'rendition',\n",
       " 'replied.The',\n",
       " 'respect',\n",
       " 'respective',\n",
       " 'respectively.Now',\n",
       " 'response',\n",
       " 'restart',\n",
       " 'results',\n",
       " 'retweeted',\n",
       " 'rewarded',\n",
       " \"rewarded'It\",\n",
       " 'ride',\n",
       " 'riding',\n",
       " 'right',\n",
       " 'role',\n",
       " 'rough',\n",
       " 'row',\n",
       " 'saga',\n",
       " 'said',\n",
       " 'said.And',\n",
       " 'said.Miley',\n",
       " 'sales',\n",
       " 'say',\n",
       " 'scheduled',\n",
       " 'script',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seemed',\n",
       " 'select',\n",
       " 'sell',\n",
       " 'series',\n",
       " 'seriously.Users',\n",
       " 'set',\n",
       " 'seven',\n",
       " 'show',\n",
       " 'show.The',\n",
       " 'showed',\n",
       " 'showing',\n",
       " 'side',\n",
       " 'similar',\n",
       " 'simmer',\n",
       " 'sing',\n",
       " 'singer',\n",
       " 'singers',\n",
       " 'single',\n",
       " 'site',\n",
       " 'site.Kim',\n",
       " 'slightly',\n",
       " 'slim',\n",
       " 'slot',\n",
       " 'slowed',\n",
       " 'smash',\n",
       " 'smooth',\n",
       " 'snubbed',\n",
       " 'solo',\n",
       " 'something',\n",
       " 'song',\n",
       " 'soon.CBS',\n",
       " 'sorry',\n",
       " 'sorts.Lady',\n",
       " 'soul-bearing',\n",
       " 'sound',\n",
       " 'sounded',\n",
       " 'speaking',\n",
       " 'speaks',\n",
       " 'special',\n",
       " 'speech',\n",
       " 'sprawling',\n",
       " 'spree.In',\n",
       " 'stage',\n",
       " 'standard',\n",
       " 'standing',\n",
       " 'stands',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'still',\n",
       " 'stopped',\n",
       " 'stormed',\n",
       " 'story',\n",
       " 'strings',\n",
       " 'strong',\n",
       " 'stunt',\n",
       " 'subjective',\n",
       " 'suggested',\n",
       " 'suggestion',\n",
       " 'supermodels',\n",
       " 'superstar',\n",
       " 'support',\n",
       " 'supporter',\n",
       " 'surprisingly',\n",
       " 'swear',\n",
       " 'swearing',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'taught',\n",
       " 'teaming',\n",
       " 'teared',\n",
       " 'telling',\n",
       " 'tells',\n",
       " 'them.Pause',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'thousands',\n",
       " 'through.The',\n",
       " 'throughout.After',\n",
       " 'time',\n",
       " 'tinny',\n",
       " 'tonight',\n",
       " 'took',\n",
       " 'top',\n",
       " 'topped',\n",
       " 'tour',\n",
       " 'trans',\n",
       " 'trends',\n",
       " 'tribute',\n",
       " 'trio',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'tweet',\n",
       " 'tweeted',\n",
       " 'tweets',\n",
       " 'twerking',\n",
       " 'twins',\n",
       " 'twist',\n",
       " 'twitter',\n",
       " 'two',\n",
       " 'two-time',\n",
       " 'u',\n",
       " 'ubiquitous',\n",
       " 'unhappy',\n",
       " 'unlike',\n",
       " 'unwittingly',\n",
       " 'uproarious',\n",
       " 'upset',\n",
       " 'us',\n",
       " 'used',\n",
       " 'user',\n",
       " 'users',\n",
       " 'using',\n",
       " 'uttered',\n",
       " 'vent',\n",
       " 'vid',\n",
       " 'video',\n",
       " 'views',\n",
       " 'volley',\n",
       " 'vote',\n",
       " 'vote.Portnow',\n",
       " 'voting',\n",
       " 'waded',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'wave.In',\n",
       " 'way',\n",
       " 'weekends',\n",
       " 'well',\n",
       " 'well-documented',\n",
       " 'went',\n",
       " 'whether',\n",
       " 'whisper-thin',\n",
       " 'white',\n",
       " 'widely',\n",
       " 'wielding',\n",
       " 'winners',\n",
       " 'winning',\n",
       " 'within',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'word',\n",
       " 'work',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worldwide',\n",
       " 'would',\n",
       " 'writer',\n",
       " 'wrote',\n",
       " 'wrote.If',\n",
       " 'wrote.Unlike',\n",
       " 'year',\n",
       " 'year.Adele',\n",
       " 'year.It',\n",
       " 'year.The',\n",
       " 'years',\n",
       " 'years.The'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
