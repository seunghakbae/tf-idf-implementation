{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모든 document를 저장할 수 있는 text_set 선언\n",
    "text_set = [] \n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "text_dict = defaultdict(list) # 각 text의 내용과 index를 저장하는 defaultdict\n",
    "\n",
    "# open the folder where documents are saved, and save all the documents\n",
    "path = 'data/'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    file = open(os.path.join(path,filename), 'r', encoding='utf-8')\n",
    "    text = file.readlines() # 내용을 읽는다. \n",
    "    filename_int = int(filename.replace(\".txt\", \"\")) # 파일 이름에서 .txt를 지우고 index값을 int로 바꾼다. \n",
    "    text_dict[filename_int] = text \n",
    "    #text_set.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 4, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 5, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_text_dict = collections.OrderedDict(sorted(text_dict.items())) # index별로 sort한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_text_dict.keys() # 제대로 sort됬는지 본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ordered_text_dict: #text_set에 넣는다. \n",
    "    text_set.append(ordered_text_dict[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (CNN)If you remember Miley twerking or Kanye\\'s mic-snatching, you\\'ll know music awards can get chaotic.Like on Tuesday, when singer Taylor Swift took a Twitter dig at hip-hop queen Nicki Minaj and the Internet erupted into a sprawling conversation about race and gender. And then Kim Kardashian got involved. Stay with us.\\'Black women are rarely rewarded\\'It started when Minaj took to her keyboard to vent after one of her biggest hits wasn\\'t nominated for video of the year for the MTV Video Music Awards.Mind you, this isn\\'t any old clip. \"Anaconda,\" an uproarious, raunchy twist on pop culture portrayals of curvy black women, received 19.6 million clicks in its first 24 hours, a music video record at the time. So the rapper had a reason to be upset when she was snubbed -- and she made her reasoning clear.\"If I was a different \\'kind\\' of artist, Anaconda would be nominated for best choreo and vid of the year as well,\" said Minaj.\"Black women influence pop culture so much but are rarely rewarded for it.\"She then tweeted: \"If your video celebrates women with very slim bodies, you will be nominated for vid of the year.\"That\\'s when Taylor Swift -- who describes herself as a feminist -- jumped in, apparently taking the tweets personally.\"I\\'ve done nothing but love & support you. It\\'s unlike you to pit women against each other. Maybe one of the men took your slot,\" she wrote.Unlike \"Anaconda,\" Swift\\'s video for \"Bad Blood\" was nominated for the top award. Featuring whisper-thin supermodels wielding guns, the pseudo-action movie dethroned Minaj\\'s video for most views in one day when it was released this year.It was a surprisingly barbed volley from a singer known for her meticulously crafted public image -- even Nicki Minaj seemed taken aback. \"Didn\\'t say a word about u,\" she replied.The rapper then retweeted a supporter who accused Taylor Swift of not being \"critical of racist media.\"Swift followed up with what seemed to be an attempt to smooth things over. \"You\\'re invited to any stage I\\'m ever on,\" she said.Miley Cyrus to host VMAsA double standard?Twitter wasn\\'t buying it: As \"Nicki\" and \"Taylor\" became worldwide trends, thousands of users came to Nicki Minaj\\'s defense.Many said the stars\\' exchange showed a double standard: Swift was widely praised as brave for speaking up about problems in the music industry last month -- now Minaj was getting lashed for doing the same. Nicki was making an important point, they argued: Not all women\\'s experiences are the same, but black women don\\'t often get taken as seriously.Users expressed disapproval with the contrasting way some media portrayed the respective artists: Taylor Swift -- beautiful, pure; Nicki Minaj -- hysterical, angry.Janet Mock, a writer and trans woman activist, suggested flipping the script, posting a picture of an angry Swift and a calm Minaj.It wasn\\'t long before at least one publication -- Entertainment Weekly -- apologized for their \"insensitive juxtaposition of photos.\"For many users, Swift and Minaj\\'s interaction reflected an all-too-familiar dynamic: A black woman makes a point, and a white woman tells her to simmer down or hijacks the conversation in the name of feminism.\"A black woman speaks up on racial problems within the music industry. A white woman makes it completely about herself,\" said user XLNB.Supporters of Taylor Swift began defending the singer with the hashtag #TeamTaylor. Some argued Beyonce\\'s nomination meant Nicki\\'s omission couldn\\'t have been about race.But at day\\'s end there were more than double the amount of tweets using the hashtag #TeamNicki, according to Topsy, a Twitter analytics site.Kim Kardashian, Bruno Mars join the frayThen, the conversation took a slightly absurd turn as Kim Kardashian waded into the story -- unwittingly, she claims.As the Nicki and Taylor saga continued to ride high in Twitter\\'s trends, the reality show star tweeted a photo of herself with the caption, \"Imma let you finish but...\"For those of you who missed the reference: Kanye West -- Kim Kardashian\\'s husband -- uttered that phrase when he grabbed the microphone from Taylor Swift at the MTV Music Video Awards in 2009. So it sounded like Kim was telling Taylor to back off. Something she frantically denied once users started pointing out the connection.\"Wait wait I\\'m in Paris it\\'s the middle of the night & I\\'m posting my Vogue Spain pics not having a clue what\\'s going on in the music world,\" she wrote.If that weren\\'t enough drama for an evening, rapper Meek Mill, Nicki Minaj\\'s boyfriend, decided before midnight that it would be a perfect time to launch an attack on superstar hip-hop artist Drake.But in the end, it was mild-mannered Bruno Mars, a video of the year nominee, who may have won the night, with a joking provocation of fellow nominee Ed Sheeran.\"Yo I want in on this twitter Beef!! VMAs is the new WWF!! @edsheeran F*** You!\" he said.And that is how the Internet works.'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_set[0][0] # 체크한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Flow\n",
    "\n",
    "- 1. 각 text별로 tokenize를 한다. \n",
    "- 2. token에서 stopwords나 필요없는 부분을 제거하고 stemming을 해준다. \n",
    "- 3. 각 document, 각 token별 token 몇개 있는지 구한 다음, table로 저장한다. \n",
    "- 4. 각 token별 어떤 document가 있는 지 inverted index를 구한다. \n",
    "- 5. tf-idf를 구한다.\n",
    "- 6. query가 속한 document들만 가져온다. \n",
    "- 7. query의 tf-idf를 구한다. \n",
    "- 8. cosine-similarity를 구한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Preprocessing\n",
    "\n",
    "### 1. 각 text별로 tokenize를 한다. \n",
    "### 2. token에서 stopwords나 필요없는 부분을 제거하고 stemming을 해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords 불러오기.\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 소문자만 걸러내기 위한 regex\n",
    "regex = re.compile('^[a-z]*[a-z]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 9), match='president'>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.match('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set() # 모든 단어의 set\n",
    "vocab_list = [] # 각 document별로 단어.\n",
    "porter_stemmer = PorterStemmer() # stemmer\n",
    "\n",
    "#for i in range(len(text_set)):\n",
    "for i in range(len(text_set)):\n",
    "    tokenized_temp = word_tokenize(text_set[i][0].lower()) # 소문자로 만든 뒤 tokenized를 했다. \n",
    "    split_tokenized = [] # hypen이나 .으로 연결되어 있는 token들을 포함하여 전체적인 token을 일시적으로 포함하는 list.\n",
    "        \n",
    "    for token in tokenized_temp:\n",
    "        if '-' in token: # '-'으로 연결되어 있으면 나눠준다. \n",
    "            split_tokens = token.split('-')\n",
    "            for split_token in split_tokens:\n",
    "                split_tokenized.append(split_token)\n",
    "        elif '.' in token: # '.'으로 연결되어 있으면 나눠준다. \n",
    "            split_tokens = token.split('.')\n",
    "            for split_token in split_tokens:\n",
    "                split_tokenized.append(split_token)\n",
    "        else: # 해당되지 않으면 넣는다. \n",
    "            split_tokenized.append(token)\n",
    "    \n",
    "    \n",
    "    # stopwords가 아닌것과 알파벳인것만 vocab_list에 넣는다. \n",
    "    vocab_list.append([porter_stemmer.stem(token) for token in split_tokenized if token not in stop and regex.match(token)])\n",
    "    \n",
    "    \n",
    "    # 각 vocab을 vocab_set에 넣는다. \n",
    "    for vocab in vocab_list[i]:\n",
    "        vocab_set.add(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_temp = word_tokenize(text_set[30][0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cnn',\n",
       " 'one',\n",
       " 'famou',\n",
       " 'peopl',\n",
       " 'us',\n",
       " 'tiger',\n",
       " 'wood',\n",
       " 'urg',\n",
       " 'compatriot',\n",
       " 'unit',\n",
       " 'turbul',\n",
       " 'time',\n",
       " 'countri',\n",
       " 'arguabl',\n",
       " 'recent',\n",
       " 'experienc',\n",
       " 'former',\n",
       " 'world',\n",
       " 'golfer',\n",
       " 'encourag',\n",
       " 'american',\n",
       " 'come',\n",
       " 'togeth',\n",
       " 'best',\n",
       " 'countri',\n",
       " 'speak',\n",
       " 'cnn',\n",
       " 'live',\n",
       " 'golf',\n",
       " 'show',\n",
       " 'dubaiwood',\n",
       " 'talk',\n",
       " 'aftermath',\n",
       " 'presid',\n",
       " 'trump',\n",
       " 'controversi',\n",
       " 'immigr',\n",
       " 'ban',\n",
       " 'caus',\n",
       " 'ruction',\n",
       " 'us',\n",
       " 'throughout',\n",
       " 'rest',\n",
       " 'world',\n",
       " 'trump',\n",
       " 'order',\n",
       " 'suspend',\n",
       " 'us',\n",
       " 'refuge',\n",
       " 'system',\n",
       " 'day',\n",
       " 'suspend',\n",
       " 'syrian',\n",
       " 'refuge',\n",
       " 'program',\n",
       " 'indefinit',\n",
       " 'ban',\n",
       " 'entri',\n",
       " 'seven',\n",
       " 'muslim',\n",
       " 'countri',\n",
       " 'iran',\n",
       " 'iraq',\n",
       " 'libya',\n",
       " 'somalia',\n",
       " 'sudan',\n",
       " 'syria',\n",
       " 'yemen',\n",
       " 'day',\n",
       " 'among',\n",
       " 'thing',\n",
       " 'need',\n",
       " 'unit',\n",
       " 'american',\n",
       " 'said',\n",
       " 'wood',\n",
       " 'play',\n",
       " 'golf',\n",
       " 'presid',\n",
       " 'elect',\n",
       " 'decemb',\n",
       " 'know',\n",
       " 'lot',\n",
       " 'divis',\n",
       " 'right',\n",
       " 'time',\n",
       " 'patienc',\n",
       " 'uniti',\n",
       " 'think',\n",
       " 'win',\n",
       " 'year',\n",
       " 'old',\n",
       " 'make',\n",
       " 'second',\n",
       " 'appear',\n",
       " 'year',\n",
       " 'dubai',\n",
       " 'week',\n",
       " 'fear',\n",
       " 'would',\n",
       " 'never',\n",
       " 'play',\n",
       " 'golf',\n",
       " 'dark',\n",
       " 'time',\n",
       " 'recov',\n",
       " 'multipl',\n",
       " 'back',\n",
       " 'surgeri',\n",
       " 'late',\n",
       " 'tough',\n",
       " 'tough',\n",
       " 'road',\n",
       " 'said',\n",
       " 'wood',\n",
       " 'return',\n",
       " 'golf',\n",
       " 'decemb',\n",
       " 'month',\n",
       " 'lot',\n",
       " 'dark',\n",
       " 'time',\n",
       " 'could',\n",
       " 'get',\n",
       " 'bed',\n",
       " 'could',\n",
       " 'move',\n",
       " 'pain',\n",
       " 'great',\n",
       " 'anyon',\n",
       " 'ever',\n",
       " 'nerv',\n",
       " 'pain',\n",
       " 'back',\n",
       " 'certainli',\n",
       " 'understand',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'honestli',\n",
       " 'know',\n",
       " 'time',\n",
       " 'last',\n",
       " 'year',\n",
       " 'know',\n",
       " 'ever',\n",
       " 'play',\n",
       " 'golf',\n",
       " 'fact',\n",
       " 'nerv',\n",
       " 'pain',\n",
       " 'wood',\n",
       " 'also',\n",
       " 'miss',\n",
       " 'sever',\n",
       " 'spell',\n",
       " 'knee',\n",
       " 'surgeri',\n",
       " 'notabl',\n",
       " 'win',\n",
       " 'last',\n",
       " 'major',\n",
       " 'titl',\n",
       " 'know',\n",
       " 'damag',\n",
       " 'knee',\n",
       " 'oper',\n",
       " 'knee',\n",
       " 'give',\n",
       " 'day',\n",
       " 'week',\n",
       " 'said',\n",
       " 'rather',\n",
       " 'everi',\n",
       " 'day',\n",
       " 'deal',\n",
       " 'pain',\n",
       " 'deal',\n",
       " 'nerv',\n",
       " 'pain',\n",
       " 'like',\n",
       " 'hit',\n",
       " 'funni',\n",
       " 'bone',\n",
       " 'thousand',\n",
       " 'time',\n",
       " 'day',\n",
       " 'becom',\n",
       " 'funni',\n",
       " 'play',\n",
       " 'week',\n",
       " 'dubai',\n",
       " 'desert',\n",
       " 'classic',\n",
       " 'european',\n",
       " 'tour',\n",
       " 'second',\n",
       " 'tournament',\n",
       " 'row',\n",
       " 'miss',\n",
       " 'cut',\n",
       " 'season',\n",
       " 'open',\n",
       " 'california',\n",
       " 'last',\n",
       " 'week',\n",
       " 'californian',\n",
       " 'say',\n",
       " 'rusti',\n",
       " 'torrey',\n",
       " 'pine',\n",
       " 'second',\n",
       " 'event',\n",
       " 'back',\n",
       " 'follow',\n",
       " 'decemb',\n",
       " 'hero',\n",
       " 'world',\n",
       " 'challeng',\n",
       " 'unconcern',\n",
       " 'peopl',\n",
       " 'think',\n",
       " 'techniqu',\n",
       " 'undergon',\n",
       " 'variou',\n",
       " 'reincarn',\n",
       " 'year',\n",
       " 'accommod',\n",
       " 'injuri',\n",
       " 'read',\n",
       " 'wood',\n",
       " 'toil',\n",
       " 'pga',\n",
       " 'tour',\n",
       " 'comeback',\n",
       " 'miss',\n",
       " 'cutread',\n",
       " 'wood',\n",
       " 'trump',\n",
       " 'play',\n",
       " 'golf',\n",
       " 'play',\n",
       " 'away',\n",
       " 'pain',\n",
       " 'simpl',\n",
       " 'said',\n",
       " 'wood',\n",
       " 'whatev',\n",
       " 'swing',\n",
       " 'look',\n",
       " 'like',\n",
       " 'look',\n",
       " 'aw',\n",
       " 'fine',\n",
       " 'care',\n",
       " 'look',\n",
       " 'like',\n",
       " 'jim',\n",
       " 'furyk',\n",
       " 'fine',\n",
       " 'ad',\n",
       " 'wood',\n",
       " 'refer',\n",
       " 'ryder',\n",
       " 'cup',\n",
       " 'captain',\n",
       " 'whose',\n",
       " 'unconvent',\n",
       " 'swing',\n",
       " 'liken',\n",
       " 'octopu',\n",
       " 'fall',\n",
       " 'tree',\n",
       " 'among',\n",
       " 'thing',\n",
       " 'though',\n",
       " 'net',\n",
       " 'us',\n",
       " 'open',\n",
       " 'pga',\n",
       " 'tour',\n",
       " 'win',\n",
       " 'long',\n",
       " 'pain',\n",
       " 'play',\n",
       " 'chase',\n",
       " 'recordswood',\n",
       " 'insist',\n",
       " 'competit',\n",
       " 'fire',\n",
       " 'burn',\n",
       " 'fierc',\n",
       " 'despit',\n",
       " 'advanc',\n",
       " 'year',\n",
       " 'time',\n",
       " 'away',\n",
       " 'game',\n",
       " 'chang',\n",
       " 'said',\n",
       " 'part',\n",
       " 'still',\n",
       " 'need',\n",
       " 'get',\n",
       " 'play',\n",
       " 'enough',\n",
       " 'tournament',\n",
       " 'round',\n",
       " 'hit',\n",
       " 'tournament',\n",
       " 'shot',\n",
       " 'hit',\n",
       " 'shot',\n",
       " 'buddi',\n",
       " 'back',\n",
       " 'home',\n",
       " 'hit',\n",
       " 'tournament',\n",
       " 'shot',\n",
       " 'wood',\n",
       " 'also',\n",
       " 'schedul',\n",
       " 'play',\n",
       " 'genesi',\n",
       " 'open',\n",
       " 'california',\n",
       " 'februari',\n",
       " 'honda',\n",
       " 'classic',\n",
       " 'florida',\n",
       " 'februari',\n",
       " 'build',\n",
       " 'return',\n",
       " 'augusta',\n",
       " 'master',\n",
       " 'april',\n",
       " 'read',\n",
       " 'year',\n",
       " 'tiger',\n",
       " 'wood',\n",
       " 'went',\n",
       " 'right',\n",
       " 'wrongread',\n",
       " 'jack',\n",
       " 'nicklau',\n",
       " 'major',\n",
       " 'record',\n",
       " 'untouch',\n",
       " 'last',\n",
       " 'four',\n",
       " 'green',\n",
       " 'jacket',\n",
       " 'seven',\n",
       " 'top',\n",
       " 'six',\n",
       " 'finish',\n",
       " 'nine',\n",
       " 'appear',\n",
       " 'sinc',\n",
       " 'wood',\n",
       " 'pga',\n",
       " 'tour',\n",
       " 'titl',\n",
       " 'second',\n",
       " 'sam',\n",
       " 'snead',\n",
       " 'record',\n",
       " 'none',\n",
       " 'sinc',\n",
       " 'clinch',\n",
       " 'five',\n",
       " 'win',\n",
       " 'visit',\n",
       " 'cnn',\n",
       " 'news',\n",
       " 'videosh',\n",
       " 'remain',\n",
       " 'four',\n",
       " 'major',\n",
       " 'titl',\n",
       " 'behind',\n",
       " 'jack',\n",
       " 'nicklau',\n",
       " 'record']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4091"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix\n",
    "\n",
    "### 3. 각 document, 각 token별 token 몇개 있는지 구한 다음, table로 저장한다. \n",
    "### 4. 각 token별 어떤 document가 있는 지 inverted index를 구한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import collections\n",
    "\n",
    "doc_dictionary_list = [] # 각 document별 각 word별로 몇 개 있는 지를 저장하는 dictionary를 모아놓은 list\n",
    "\n",
    "for i in range(len(vocab_list)):    \n",
    "    doc_dictionary = defaultdict(int) # 각 document별로 word가 몇 개 있는 지 저장되어 있다. \n",
    "\n",
    "    for vocab in vocab_list[i]: # vocab_list에 단어가 몇개가 있는 지 저장 한다. \n",
    "        doc_dictionary[vocab] += 1\n",
    "\n",
    "    for vocab in vocab_set: # 그리고 나머지는 다 0으로 채운다. \n",
    "        if doc_dictionary[vocab] != 0:\n",
    "            pass\n",
    "    \n",
    "    od = collections.OrderedDict(sorted(doc_dictionary.items())) # keyword들을 sort한다. \n",
    "    \n",
    "    doc_dictionary_list.append(od) # 마지막으로 lsit에 저장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_dic = defaultdict(list) # inverted_index를 포함하는 dictionary\n",
    "\n",
    "for vocab in vocab_set: # 만약 doc_dictionary에서 그 단어가 1이상 나오면 그 document의 index를 저장한다. \n",
    "    index = 0\n",
    "    for doc_dictionary in doc_dictionary_list:\n",
    "        if doc_dictionary[vocab] >= 1:\n",
    "            inverted_index_dic[vocab].append(index)\n",
    "    \n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf calcluation\n",
    "\n",
    "### 5. tf-idf를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array = list()\n",
    "\n",
    "for value in doc_dictionary_list[0].values(): # 제일 첫번째 같은 경우 numpy_array_stack에다가 저장한다. \n",
    "    array.append(value)\n",
    "\n",
    "numpy_array_stack = np.array(array) \n",
    "\n",
    "for i in range(1, len(doc_dictionary_list)):#1번부터 60번까지 하나씩 numpy_array_stack에 저장한다. \n",
    "    array = list()\n",
    "\n",
    "    for value in doc_dictionary_list[i].values():\n",
    "        array.append(value)\n",
    "\n",
    "    numpy_array = np.array(array)\n",
    "    \n",
    "    numpy_array_stack = np.vstack((numpy_array_stack, numpy_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.sum(numpy_array_stack, axis=0)\n",
    "N = len(numpy_array_stack)\n",
    "tf = numpy_array_stack + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf를 구한다. \n",
    "tf_idf = np.log(tf + 1) * np.log(N / df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query\n",
    "\n",
    "### 6. query가 속한 document들만 가져온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('query.txt', 'r') as f:\n",
    "    query = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_word = query.split(' ') # query문을 token별로 자른다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_set = set() # \n",
    "\n",
    "for word in query_word:\n",
    "    temp = inverted_index_dic[porter_stemmer.stem(word)]\n",
    "    for doc_index in temp:\n",
    "        query_doc_set.add(doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6,\n",
       " 7,\n",
       " 9,\n",
       " 14,\n",
       " 17,\n",
       " 30,\n",
       " 36,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_doc_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.83798339 2.83798339 2.07648338 ... 4.49809725 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 4.8214451  ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[4.49809725 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n",
      "[2.83798339 2.83798339 2.07648338 ... 2.83798339 2.83798339 2.83798339]\n"
     ]
    }
   ],
   "source": [
    "for index in query_doc_set:\n",
    "    print(tf_idf[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
